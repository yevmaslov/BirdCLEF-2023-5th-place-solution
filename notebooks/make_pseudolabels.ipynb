{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filepaths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from environment.utils import load_filepaths, load_config, seed_everything, create_run_folder, save_config\n",
    "from logger.wandb import init_wandb\n",
    "from features.text import get_tokenized_text\n",
    "from models.dataset import CustomDataset\n",
    "from models.model import CustomModel\n",
    "from models.trainer import Trainer\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from models.utils import get_valid_steps\n",
    "from models.optimizer import get_optimizer\n",
    "from models.scheduler import get_scheduler\n",
    "from logger.logger import Logger\n",
    "\n",
    "\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.dataset import CustomDataset\n",
    "from models.model import TattakaModel\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "from data.load_data import *\n",
    "from pathlib import Path\n",
    "\n",
    "from models.spec_transforms import *\n",
    "from models.audio_transforms import *\n",
    "from models.transforms_utils import *\n",
    "import random\n",
    "\n",
    "import timm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.utils import batch_to_device\n",
    "\n",
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    frame_predictions = []\n",
    "\n",
    "    for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred, output_frame = model(batch)\n",
    "        \n",
    "        logits = y_pred.detach().to('cpu').numpy()\n",
    "        output_frame = torch.sigmoid(output_frame).detach().to('cpu').numpy()\n",
    "        \n",
    "        predictions.append(logits)\n",
    "        frame_predictions.append(output_frame)\n",
    "\n",
    "    predictions = np.concatenate(predictions)\n",
    "    frame_predictions = np.concatenate(frame_predictions)\n",
    "    return predictions, frame_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 58.466679,
     "end_time": "2022-04-22T06:01:17.158088",
     "exception": false,
     "start_time": "2022-04-22T06:00:18.691409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, config):\n",
    "        self.dataframe = dataframe\n",
    "        self.config = config\n",
    "        self.sr = self.config.dataset.sample_rate\n",
    "\n",
    "        self.audio_transforms = get_audio_transforms(config, False)\n",
    "        self.spectrogram_transforms = get_spectrogram_transforms(config, False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        fp = self.dataframe.iloc[item].path\n",
    "        duration = self.dataframe.iloc[item].duration\n",
    "        start = self.dataframe.iloc[item].start\n",
    "        stop = self.dataframe.iloc[item].end\n",
    "\n",
    "        audio, sr = self.load_sample_wav(fp, start, stop)\n",
    "        audio = audio.astype('float32')\n",
    "        audio = np.nan_to_num(audio)\n",
    "        \n",
    "        audio, _, _, _ = self.audio_transforms(audio, None, None, None)\n",
    "        audio = np.nan_to_num(audio)\n",
    "\n",
    "        spec = self.spectrogram_transforms(torch.tensor(audio).view(1, len(audio)))\n",
    "        # spec = np.nan_to_num(spec)\n",
    "        return spec\n",
    "\n",
    "    def load_sample_wav(self, fp, start, stop):\n",
    "        wav, sr = sf.read(fp, start=start, stop=stop)\n",
    "        if len(wav) > 1:\n",
    "            wav = librosa.to_mono(wav)\n",
    "        return wav, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gem_freq(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), 1)).pow(1.0 / p)\n",
    "\n",
    "\n",
    "class GeMFreq(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = torch.nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gem_freq(x, p=self.p, eps=self.eps)\n",
    "\n",
    "\n",
    "class AttHead(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_chans, p=0.5, num_class=264, train_period=15.0, infer_period=5.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_period = train_period\n",
    "        self.infer_period = infer_period\n",
    "        self.pooling = GeMFreq()\n",
    "\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Dropout(p / 2),\n",
    "            nn.Linear(in_chans, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "        self.attention = nn.Conv1d(\n",
    "            in_channels=512,\n",
    "            out_channels=num_class,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.fix_scale = nn.Conv1d(\n",
    "            in_channels=512,\n",
    "            out_channels=num_class,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, feat):\n",
    "        feat = self.pooling(feat).squeeze(-2).permute(0, 2, 1)  # (bs, time, ch)\n",
    "\n",
    "        feat = self.dense_layers(feat).permute(0, 2, 1)  # (bs, 512, time)\n",
    "        # print(feat.shape)\n",
    "        \n",
    "        time_att = torch.tanh(self.attention(feat))\n",
    "        \n",
    "        assert self.train_period >= self.infer_period\n",
    "        \n",
    "        if self.training or self.train_period == self.infer_period: # or True\n",
    "            # print('train')\n",
    "            clipwise_pred = torch.sum(\n",
    "                torch.sigmoid(self.fix_scale(feat)) * torch.softmax(time_att, dim=-1),\n",
    "                dim=-1,\n",
    "            )  # sum((bs, 24, time), -1) -> (bs, 24)\n",
    "            logits = torch.sum(\n",
    "                self.fix_scale(feat) * torch.softmax(time_att, dim=-1),\n",
    "                dim=-1,\n",
    "            )\n",
    "        else:\n",
    "            # print('eval')\n",
    "            clipwise_pred_long = torch.sum(\n",
    "                torch.sigmoid(self.fix_scale(feat)) * torch.softmax(time_att, dim=-1),\n",
    "                dim=-1,\n",
    "            )  # sum((bs, 24, time), -1) -> (bs, 24)\n",
    "            \n",
    "            feat_time = feat.size(-1)\n",
    "            start = feat_time / 2 - feat_time * (self.infer_period / self.train_period) / 2\n",
    "            end = start + feat_time * (self.infer_period / self.train_period)\n",
    "            \n",
    "            start = int(start)\n",
    "            end = int(end)\n",
    "            \n",
    "            feat = feat[:, :, start:end]\n",
    "            att = torch.softmax(time_att[:, :, start:end], dim=-1)\n",
    "            \n",
    "            # print(feat.shape)\n",
    "            \n",
    "            clipwise_pred = torch.sum(\n",
    "                torch.sigmoid(self.fix_scale(feat)) * att,\n",
    "                dim=-1,\n",
    "            )\n",
    "            logits = torch.sum(\n",
    "                self.fix_scale(feat) * att,\n",
    "                dim=-1,\n",
    "            )\n",
    "            time_att = time_att[:, :, start:end]\n",
    "        return (\n",
    "            logits,\n",
    "            clipwise_pred,\n",
    "            self.fix_scale(feat).permute(0, 2, 1),\n",
    "            time_att.permute(0, 2, 1),\n",
    "        )\n",
    "\n",
    "    \n",
    "def get_timm_backbone(config, pretrained):\n",
    "    backbone = timm.create_model(\n",
    "        config.model.backbone_type,\n",
    "        pretrained=pretrained,\n",
    "        num_classes=0,\n",
    "        global_pool=\"\",\n",
    "        in_chans=1,\n",
    "    )\n",
    "    return backbone\n",
    "\n",
    "\n",
    "class TattakaModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        pretrained,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.model = get_timm_backbone(config, pretrained)\n",
    "        self.model = timm.create_model(\n",
    "            config.model.backbone_type, features_only=True, pretrained=False, in_chans=1\n",
    "        )\n",
    "        encoder_channels = self.model.feature_info.channels()\n",
    "        dense_input = encoder_channels[-1]\n",
    "        self.head = AttHead(\n",
    "            dense_input,\n",
    "            p=config.model.dropout,\n",
    "            num_class=len(config.dataset.labels),\n",
    "            train_period=config.dataset.train_duration,\n",
    "            infer_period=config.dataset.valid_duration,\n",
    "        )\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, images):\n",
    "        spec = images\n",
    "        \n",
    "        feats = self.model(spec)\n",
    "        logits, output_clip, output_frame, output_attention = self.head(feats[-1])\n",
    "        \n",
    "        return output_clip, output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def padded_cmap(solution, submission, padding_factor=5):\n",
    "    solution = solution#.drop(['row_id'], axis=1, errors='ignore')\n",
    "    submission = submission#.drop(['row_id'], axis=1, errors='ignore')\n",
    "    new_rows = []\n",
    "    for i in range(padding_factor):\n",
    "        new_rows.append([1 for i in range(len(solution.columns))])\n",
    "    new_rows = pd.DataFrame(new_rows)\n",
    "    new_rows.columns = solution.columns\n",
    "    padded_solution = pd.concat([solution, new_rows]).reset_index(drop=True).copy()\n",
    "    padded_submission = pd.concat([submission, new_rows]).reset_index(drop=True).copy()\n",
    "    score = sklearn.metrics.average_precision_score(\n",
    "        padded_solution.values,\n",
    "        padded_submission.values,\n",
    "        average='macro',\n",
    "    )\n",
    "    return score\n",
    "\n",
    "\n",
    "def map_score(solution, submission):\n",
    "    solution = solution#.drop(['row_id'], axis=1, errors='ignore')\n",
    "    submission = submission#.drop(['row_id'], axis=1, errors='ignore')\n",
    "    score = sklearn.metrics.average_precision_score(\n",
    "        solution.values,\n",
    "        submission.values,\n",
    "        average='micro',\n",
    "    )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filepaths = load_filepaths('../filepaths.yaml')\n",
    "config = load_config('../config.yaml')\n",
    "config.dataset.train_duration = 5\n",
    "config.dataset.valid_duration = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XenoCanto 2023 shape:  (25545, 17)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(filepaths.raw_dir / 'train_metadata.csv')\n",
    "folds = pd.read_csv(filepaths.processed_dir / 'birdclef-2023' / 'folds.csv')\n",
    "duration = pd.read_csv(filepaths.processed_dir / 'birdclef-2023' / 'duration.csv')\n",
    "\n",
    "labels = sorted(train.primary_label.unique())\n",
    "config.dataset.labels = labels\n",
    "\n",
    "train = pd.merge(train, folds, on='url', how='left')\n",
    "train = pd.merge(train, duration, on='url', how='left')\n",
    "\n",
    "train_audio_dir = filepaths.raw_dir / 'train_audio'\n",
    "train['filepath'] = train_audio_dir / train['filename']\n",
    "\n",
    "train_temp = train.copy()\n",
    "\n",
    "for label in labels:\n",
    "    primary = train['primary_label'] == label\n",
    "    secondary = train['secondary_labels'].apply(lambda x: label in x)\n",
    "    train[label] = (primary | secondary).astype(int)\n",
    "    \n",
    "train[\"weight\"] = np.clip(train[\"rating\"] / train[\"rating\"].max(), 0.1, 1.0)\n",
    "train['filepath'] = filepaths.raw_dir / 'train_audio' / train['filename']\n",
    "\n",
    "xc_2023_data = load_xc_data(config.dataframe.xc_2023_data_config, filepaths, year=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XenoCanto 2023 shape:  (56204, 16)\n"
     ]
    }
   ],
   "source": [
    "year = 2023\n",
    "\n",
    "xc_data = pd.read_csv(filepaths.processed_dir / f'birdclef-{year}_XenoCanto' / 'metadata.csv')\n",
    "xc_data['filename'] = xc_data['filename'].str.replace('.mp3', '.wav')\n",
    "\n",
    "fp = Path('../data/external/birdclef-2023-africa/wav')\n",
    "xc_data['filepath'] = fp / xc_data['filename']\n",
    "\n",
    "xc_data['rating'].fillna(1, inplace=True)\n",
    "\n",
    "# if 'duration' in xc_data.columns:\n",
    "#     xc_data.drop(columns=['duration'], inplace=True)\n",
    "    \n",
    "xc_data = xc_data[xc_data['primary_label'] == 'africa'].reset_index(drop=True)\n",
    "\n",
    "# duration = pd.read_csv('../data/external/birdclef-2023-africa/duration.csv')\n",
    "\n",
    "# xc_data['url'] = xc_data['url'].apply(preprocess_url)\n",
    "# duration['url'] = duration['url'].apply(preprocess_url)\n",
    "\n",
    "# xc_data = pd.merge(xc_data, duration, on=['url', 'filename'], how='left')\n",
    "# xc_data = xc_data[xc_data['duration'] >= 1].reset_index(drop=True)\n",
    "\n",
    "xc_data['fold'] = 99\n",
    "\n",
    "xc_data = xc_data.reset_index(drop=True)\n",
    "xc_data['duration'] = xc_data['duration'].astype(int)\n",
    "xc_data['duration_float'] = xc_data['duration']\n",
    "\n",
    "print(f'XenoCanto {year} shape: ', xc_data.shape)\n",
    "\n",
    "def update_max_duration(df, max_duration=180):\n",
    "    df['md'] = max_duration\n",
    "    df['duration'] = df[['duration', 'md']].min(axis=1)\n",
    "    df = df.drop('md', axis=1)\n",
    "    return df\n",
    "\n",
    "xc_2023_data = update_max_duration(xc_2023_data, max_duration=120)\n",
    "accepted_licenses = [\n",
    "    # '//creativecommons.org/licenses/by-nc-nd/4.0/',\n",
    "    '//creativecommons.org/licenses/by-nc-sa/4.0/',\n",
    "    # '//creativecommons.org/licenses/by-nc-nd/2.5/',\n",
    "    # '//creativecommons.org/licenses/by-nc-nd/3.0/',\n",
    "    '//creativecommons.org/licenses/by-nc-sa/3.0/',\n",
    "    # '//creativecommons.org/publicdomain/zero/1.0/',\n",
    "    '//creativecommons.org/licenses/by-sa/4.0/',\n",
    "    '//creativecommons.org/licenses/by/4.0/',\n",
    "    '//creativecommons.org/licenses/by-nc/4.0/',\n",
    "    '//creativecommons.org/licenses/by-sa/3.0/',\n",
    "]\n",
    "\n",
    "xc_data['fn'] = xc_data['filename'].apply(lambda x: x.split('_')[0])\n",
    "xc_data = xc_data[xc_data['license'].isin(accepted_licenses)].reset_index(drop=True)\n",
    "\n",
    "xc_data.loc[xc_data['duration'] > 120, 'duration'] = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dataframe(dataframe, window_stride=1):\n",
    "    dfs = []\n",
    "    for i, row in dataframe.iterrows():\n",
    "        for j in range(0, max(1, int(row['duration'])-5+1), window_stride):\n",
    "            new_fn = row['filename'] + f'_{j}'\n",
    "            start_end = [j*32000, (j+5)*32000]\n",
    "            other_cols = [str(val) for val in row.values[1:]]\n",
    "            dfs.append([new_fn] + start_end + other_cols)\n",
    "            \n",
    "        # new_fn = row['filename'] + f'_{0}'\n",
    "        # start_end = [0*32000, 5*32000]\n",
    "        # other_cols = [str(val) for val in row.values[1:]]\n",
    "        # dfs.append([new_fn] + start_end + other_cols)\n",
    "        \n",
    "    columns = ['filename', 'start', 'end', 'path', 'duration', 'primary_label', 'secondary_labels']\n",
    "    dataframe = pd.DataFrame(dfs, columns=columns)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def get_dataloader(dataframe, config):\n",
    "    dataset = TestDataset(dataframe, config)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=12,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def load_models(exp_list, folds, final_folder):\n",
    "    models = []\n",
    "    models_names = []\n",
    "    for exp_name in exp_list:\n",
    "        for fold in folds:\n",
    "            model_folder = filepaths.models_dir / exp_name\n",
    "            if os.path.isfile(model_folder / final_folder / f'fold{fold}.parquet'):\n",
    "                continue\n",
    "            \n",
    "            folder = 'models' # if fold != -1 else 'chkp'\n",
    "            fn = f'fold_{fold}_best.pth'# if fold != -1 else f'fold_{fold}_chkp.pth'\n",
    "            state_path = model_folder / folder / fn\n",
    "            \n",
    "            config = load_config(filepaths.models_dir / exp_name / 'config.yaml')\n",
    "            config.dataset.train_duration = 5\n",
    "            config.dataset.valid_duration = 5\n",
    "            config.dataset.labels = labels\n",
    "            \n",
    "            model = TattakaModel(config, pretrained=False)\n",
    "            \n",
    "            state = torch.load(state_path, map_location=torch.device('cpu'))\n",
    "            model.load_state_dict(state['model'])\n",
    "            model.eval()\n",
    "            model.to(device)\n",
    "            models.append(model)\n",
    "            models_names.append(f'{exp_name}_{fold}')\n",
    "            \n",
    "    return models, models_names\n",
    "\n",
    "\n",
    "def make_pseudolabels(dataframe, exp_list, folds=(0,1,2,3), window_stride=1, folder='pseudolabels'):\n",
    "    test_df = dataframe[['filename', 'filepath', 'duration', 'primary_label', 'secondary_labels']].copy().reset_index(drop=True)\n",
    "    test_df.rename(columns={'filepath': 'path'}, inplace=True)\n",
    "    test_df['duration'] = test_df['duration'].astype(int)\n",
    "\n",
    "    test_df = expand_dataframe(test_df, window_stride=window_stride)\n",
    "    test_df['duration'] = test_df['duration'].astype(int)\n",
    "    \n",
    "    config = load_config(filepaths.models_dir / exp_list[0] / 'config.yaml')\n",
    "    config.dataset.train_duration = 5\n",
    "    config.dataset.valid_duration = 5\n",
    "    config.dataset.labels = labels\n",
    "    test_dl = get_dataloader(test_df, config)\n",
    "\n",
    "    models, models_names = load_models(exp_list, folds, folder)\n",
    "    if len(models) == 0:\n",
    "        return True\n",
    "\n",
    "    predictions = {model_name: [] for model_name in models_names}\n",
    "    for step, batch in tqdm(enumerate(test_dl), total=len(test_dl)):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        for model_name, model in zip(models_names, models):\n",
    "            with torch.no_grad():\n",
    "                y_pred, output_frame = model(batch)\n",
    "            logits = y_pred.detach().to('cpu').numpy()\n",
    "            predictions[model_name].append(logits)\n",
    "\n",
    "    for model_name, logits in predictions.items():\n",
    "        exp_name, fold = model_name.split('_')\n",
    "        fp = filepaths.models_dir / exp_name / folder / f'fold{fold}.parquet'\n",
    "\n",
    "        preds = np.round(np.concatenate(logits), 3)\n",
    "        test_df[config.dataset.labels] = preds\n",
    "\n",
    "        if not os.path.isdir(fp.parent):\n",
    "            os.makedirs(fp.parent)\n",
    "\n",
    "        try:\n",
    "            test_df.drop(['path', 'duration', 'primary_label', 'secondary_labels'], axis=1, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        test_df.to_parquet(fp, index=False)\n",
    "    return True\n",
    "\n",
    "\n",
    "def concatenate_actual_pseudolabels(exp):\n",
    "    fp = filepaths.models_dir / exp / 'pseudolabels' / f'pseudolabels.parquet'\n",
    "    if os.path.isfile(fp):\n",
    "        return True\n",
    "    pseudolabels = []\n",
    "    for fold in tqdm(range(4)):\n",
    "        pseudo = pd.read_parquet(filepaths.models_dir / exp / 'pseudolabels' / f'fold{fold}.parquet')\n",
    "        pseudolabels.append(pseudo)\n",
    "    pseudolabels = pd.concat(pseudolabels, axis=0)\n",
    "    pseudolabels.to_parquet(fp, index=False)\n",
    "    return True\n",
    "\n",
    "\n",
    "def make_mean_pseudolabels(exp, folder):\n",
    "    new_fp = filepaths.models_dir / exp / folder / f'pseudolabels.parquet'\n",
    "    pseudolabels = None\n",
    "    for fold in tqdm(range(4)):\n",
    "        fp = filepaths.models_dir / exp / folder / f'fold{fold}.parquet'\n",
    "        pseudo = pd.read_parquet(fp)\n",
    "        if pseudolabels is None:\n",
    "            pseudolabels = pseudo[config.dataset.labels].values * 0.25\n",
    "        else:\n",
    "            pseudolabels += pseudo[config.dataset.labels].values * 0.25\n",
    "    pseudo[config.dataset.labels] = pseudolabels\n",
    "    pseudo.to_parquet(new_fp, index=False)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp_list(weights):\n",
    "    exp_list = sorted(weights.keys())\n",
    "    return exp_list\n",
    "\n",
    "\n",
    "def make_weighted_pseudolabels(weights, folds=(0, 1, 2, 3), src_folder='pseudolabels'):\n",
    "    exp_list = get_exp_list(weights)\n",
    "    folder_name = '_'.join(sorted(exp_list))\n",
    "    folder_path = filepaths.models_dir / folder_name / src_folder\n",
    "    \n",
    "    for fold in folds:\n",
    "        pseudolabels = None\n",
    "        for exp, weight in tqdm(weights.items()):\n",
    "            pseudo = pd.read_parquet(filepaths.models_dir / exp / src_folder / f'fold{fold}.parquet')\n",
    "            pseudo = pseudo[pseudo['filename'].isin(pseudo_temp.filename.values)]\n",
    "            pseudo.sort_values('filename', inplace=True)\n",
    "            try:\n",
    "                if pseudolabels is None:\n",
    "                    pseudolabels = pseudo[config.dataset.labels].values * weight\n",
    "                else:\n",
    "                    pseudolabels += pseudo[config.dataset.labels].values * weight\n",
    "            except:\n",
    "                print(exp)\n",
    "\n",
    "        df = pseudo.copy()\n",
    "        df[config.dataset.labels] = pseudolabels\n",
    "        \n",
    "        if 'africa' in src_folder:\n",
    "            df['filename'] = df['filename'].apply(lambda x: x.split('_')[0])\n",
    "            df = pd.merge(df, xc_data, on='filename', how='left')\n",
    "            df[\"weight\"] = np.clip(df[\"rating\"] / df[\"rating\"].max(), 0.1, 1.0)\n",
    "            df.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
    "            df['filepath'] = df['filepath'].astype(str)\n",
    "\n",
    "        fp = folder_path / f'fold{fold}.parquet'\n",
    "        if not os.path.isdir(fp.parent):\n",
    "            os.makedirs(fp.parent)\n",
    "        \n",
    "        df.to_parquet(fp, index=False)\n",
    "        \n",
    "    with open(folder_path / 'weights.json', \"w\") as outfile:\n",
    "        json.dump(weights, outfile)\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp114 pseudolabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df723e522be413bacb09cabe9f47824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1028 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa4ddbaec27401d9fec76425a3a1253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd1ac1d57394519ac1f93df6ba78da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69a7bebc0f64dabb517d0265a0a3996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_list = ['exp114',]\n",
    "\n",
    "for fold in [0, 1, 2, 3]:\n",
    "    _ = make_pseudolabels(\n",
    "            dataframe=train[train['fold'] == fold],\n",
    "            exp_list=exp_list,\n",
    "            folds=(fold, ),\n",
    "            window_stride=5,\n",
    "            folder='pseudolabels'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90dae9677ae649bb93423276eef3e48f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenate_actual_pseudolabels('exp114')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo = pd.read_parquet('../models/exp114/pseudolabels/pseudolabels.parquet')\n",
    "for labels in config.dataset.labels:\n",
    "    pseudo.loc[pseudo[label] >= 0.2, label] = 1\n",
    "    pseudo.loc[pseudo[label] < 0.2, label] = 0\n",
    "pseudo.to_parquet('../models/exp114/pseudolabels/pseudolabels.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp114 - Exp165 pseudolabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_list = ['exp114', 'exp145', 'exp148', 'exp150', 'exp154', 'exp160', 'exp161', 'exp162', 'exp163', 'exp164', 'exp165', ]\n",
    "\n",
    "for fold in [0, 1, 2, 3]:\n",
    "    _ = make_pseudolabels(\n",
    "            dataframe=train[train['fold'] == fold],\n",
    "            exp_list=exp_list,\n",
    "            folds=(fold, ),\n",
    "            window_stride=1,\n",
    "            folder='pseudolabels' \n",
    "        )\n",
    "\n",
    "weights = {exp: 1/len(exp_list) for exp in exp_list}\n",
    "make_weighted_pseudolabels(weights, folds=(0, 1, 2, 3), src_folder='pseudolabels')\n",
    "concatenate_actual_pseudolabels('exp114_exp145_exp148_exp150_exp154_exp160_exp161_exp162_exp163_exp164_exp165')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = xc_2023_data.copy()\n",
    "\n",
    "accepted_licenses = [\n",
    "    # '//creativecommons.org/licenses/by-nc-nd/4.0/',\n",
    "    '//creativecommons.org/licenses/by-nc-sa/4.0/',\n",
    "    # '//creativecommons.org/licenses/by-nc-nd/2.5/',\n",
    "    # '//creativecommons.org/licenses/by-nc-nd/3.0/',\n",
    "    '//creativecommons.org/licenses/by-nc-sa/3.0/',\n",
    "    # '//creativecommons.org/publicdomain/zero/1.0/',\n",
    "    '//creativecommons.org/licenses/by-sa/4.0/',\n",
    "    '//creativecommons.org/licenses/by/4.0/',\n",
    "    '//creativecommons.org/licenses/by-nc/4.0/',\n",
    "    '//creativecommons.org/licenses/by-sa/3.0/',\n",
    "]\n",
    "subset = subset[subset['license'].isin(accepted_licenses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_list = ['exp114', 'exp145', 'exp148', 'exp150', 'exp154', 'exp160', 'exp161', 'exp162', 'exp163', 'exp164', 'exp165', ]\n",
    "\n",
    "for fold in [0, 1, 2, 3]:\n",
    "    _ = make_pseudolabels(\n",
    "            dataframe=subset,\n",
    "            exp_list=exp_list,\n",
    "            folds=(fold, ),\n",
    "            window_stride=1,\n",
    "            folder='pseudolabels_xc' # only sa licenses\n",
    "        )\n",
    "    \n",
    "weights = {exp: 1/len(exp_list) for exp in exp_list}\n",
    "make_weighted_pseudolabels(weights, folds=(0, 1, 2, 3), src_folder='pseudolabels_xc')\n",
    "make_mean_pseudolabels('exp114_exp145_exp148_exp150_exp154_exp160_exp161_exp162_exp163_exp164_exp165', 'pseudolabels_xc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 169 - Exp226 pseudolabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_list = ['exp169', 'exp172', 'exp176', 'exp177', 'exp182', 'exp187', 'exp188', 'exp192', 'exp205', 'exp222', 'exp223', 'exp226', ]\n",
    "\n",
    "for fold in [0, 1, 2, 3]:\n",
    "    _ = make_pseudolabels(\n",
    "            dataframe=train,\n",
    "            exp_list=exp_list,\n",
    "            folds=(fold, ),\n",
    "            window_stride=1,\n",
    "            folder='pseudolabels_full2'\n",
    "        )\n",
    "    \n",
    "weights = {exp: 1/len(exp_list) for exp in exp_list}\n",
    "make_weighted_pseudolabels(weights, folds=(0, 1, 2, 3), src_folder='pseudolabels_full2')\n",
    "make_mean_pseudolabels('exp169_exp172_exp176_exp177_exp182_exp187_exp188_exp192_exp205_exp222_exp223_exp226', 'pseudolabels_full2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_list = ['exp169', 'exp172', 'exp176', 'exp177', 'exp182', 'exp187', 'exp188', 'exp192',]\n",
    "\n",
    "for fold in [0, 1, 2, 3]:\n",
    "    _ = make_pseudolabels(\n",
    "            dataframe=xc_2023_data,\n",
    "            exp_list=exp_list,\n",
    "            folds=(fold, ),\n",
    "            window_stride=1,\n",
    "            folder='pseudolabels_xc_full' # with nd licenses\n",
    "        )\n",
    "\n",
    "weights = {exp: 1/len(exp_list) for exp in exp_list}\n",
    "make_weighted_pseudolabels(weights, folds=(0, 1, 2, 3), src_folder='pseudolabels_xc_full')\n",
    "make_mean_pseudolabels('exp169_exp172_exp176_exp177_exp182_exp187_exp188_exp192', 'pseudolabels_xc_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_list = ['exp169', 'exp172', 'exp176', 'exp177', 'exp182', 'exp187', 'exp188', 'exp192',]\n",
    "\n",
    "for fold in [0, 1, 2, 3]:\n",
    "    _ = make_pseudolabels(\n",
    "            dataframe=xc_data,\n",
    "            exp_list=exp_list,\n",
    "            folds=(fold, ),\n",
    "            window_stride=1,\n",
    "            folder='pseudolabels_xc_africa'\n",
    "        )\n",
    "    \n",
    "weights = {exp: 1/len(exp_list) for exp in exp_list}\n",
    "make_weighted_pseudolabels(weights, folds=(0, 1, 2, 3), src_folder='pseudolabels_xc_africa')\n",
    "make_mean_pseudolabels('exp169_exp172_exp176_exp177_exp182_exp187_exp188_exp192', 'pseudolabels_xc_africa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
